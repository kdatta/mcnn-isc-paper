\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Arbabshirani2018,Akkus2017}
\citation{ciresan2013,Litjens2016,Janowczyk2016}
\citation{Kraus2017,Sommer2017,Ciresan2012}
\citation{Litjens2017}
\citation{Mattiazzi2016,Boutros2015,Singh2014,Scheeder2018,zock2009,assay2014}
\citation{Kraus2017,Godinez2017,Godinez2018,Sommer2017,Ando2017}
\citation{Buyssens2012}
\citation{Godinez2017}
\citation{jouppi2017}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {title}{Training Multiscale-CNN for Large Microscopy Image Classification in One Hour}{1}{chapter.2}}
\@writefile{toc}{\authcount {7}}
\@writefile{toc}{\contentsline {author}{Kushal Datta \and Imtiaz Hossain \and Sun Choi \and Vikram Saletore \and Kyle Ambert \and William J. Godinez \and Xian Zhang}{1}{chapter.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.2.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.2.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{[1][1][]1}}
\citation{Godinez2017}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textsf  {Operations and kernels of the M-CNN model. Convolution is abbreviated \emph  {CONV}, and Max Pooling operations are abbreviated as \emph  {MAX POOL}}\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:mcnn}{{1}{3}{\textsf {Operations and kernels of the M-CNN model. Convolution is abbreviated \emph {CONV}, and Max Pooling operations are abbreviated as \emph {MAX POOL}}\relax }{figure.caption.2}{}}
\newlabel{fig:mcnn@cref}{{[figure][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Multi-scale convolutional neural network}{3}{section.2.2}}
\newlabel{sec:mcnn}{{2}{3}{Multi-scale convolutional neural network}{section.2.2}{}}
\newlabel{sec:mcnn@cref}{{[section][2][]2}{[1][3][]3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textsf  {Activation sizes in M-CNN as a function of batch size.}\relax }}{4}{figure.caption.3}}
\newlabel{fig:batchsize}{{2}{4}{\textsf {Activation sizes in M-CNN as a function of batch size.}\relax }{figure.caption.3}{}}
\newlabel{fig:batchsize@cref}{{[figure][2][]2}{[1][4][]4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Large batch training}{4}{section.2.3}}
\newlabel{sec:training}{{3}{4}{Large batch training}{section.2.3}{}}
\newlabel{sec:training@cref}{{[section][3][]3}{[1][4][]4}}
\citation{Robbins1951}
\citation{You2017}
\newlabel{eq:sgd}{{1}{5}{Large batch training}{equation.2.3.1}{}}
\newlabel{eq:sgd@cref}{{[equation][1][]1}{[1][5][]5}}
\newlabel{eq:syncsgd}{{2}{5}{Large batch training}{equation.2.3.2}{}}
\newlabel{eq:syncsgd@cref}{{[equation][2][]2}{[1][5][]5}}
\newlabel{eq:syncsgdmw}{{3}{5}{Large batch training}{equation.2.3.3}{}}
\newlabel{eq:syncsgdmw@cref}{{[equation][3][]3}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Learning rate schedule}{5}{subsection.2.3.1}}
\newlabel{sec:lr}{{3.1}{5}{Learning rate schedule}{subsection.2.3.1}{}}
\newlabel{sec:lr@cref}{{[subsection][1][3]3.1}{[1][5][]5}}
\citation{bbbc021}
\citation{Caie1913}
\citation{Godinez2017}
\citation{bbbc021}
\citation{bbbc021}
\@writefile{toc}{\contentsline {section}{\numberline {4}Dataset}{6}{section.2.4}}
\newlabel{sec:dataset}{{4}{6}{Dataset}{section.2.4}{}}
\newlabel{sec:dataset@cref}{{[section][4][]4}{[1][6][]6}}
\citation{GoogleTPU}
\citation{Horovod}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textsf  {Example images from the BBBC021\cite  {bbbc021} dataset showing phenotypes from treatment with compound-concentration pairs with different mechanisms of action: a) DSMO (neutral control), b) Microtubule destabilizer, c) Cholesterol lowering, d) Microtubule stabilizer, e) Actin disrupter, f) Epithelial. DNA staining is shown in blue, the F-actin staining in red and the Β-tubulin staining in green. The insets show a magnified view of the same phenotypes.}\relax }}{7}{figure.caption.4}}
\newlabel{fig:bbbc_whole}{{3}{7}{\textsf {Example images from the BBBC021\cite {bbbc021} dataset showing phenotypes from treatment with compound-concentration pairs with different mechanisms of action: a) DSMO (neutral control), b) Microtubule destabilizer, c) Cholesterol lowering, d) Microtubule stabilizer, e) Actin disrupter, f) Epithelial. DNA staining is shown in blue, the F-actin staining in red and the Β-tubulin staining in green. The insets show a magnified view of the same phenotypes.}\relax }{figure.caption.4}{}}
\newlabel{fig:bbbc_whole@cref}{{[figure][3][]3}{[1][6][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Performance results}{7}{section.2.5}}
\newlabel{sec:performance}{{5}{7}{Performance results}{section.2.5}{}}
\newlabel{sec:performance@cref}{{[section][5][]5}{[1][7][]7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Experimental setup}{7}{subsection.2.5.1}}
\newlabel{sec:setup}{{5.1}{7}{Experimental setup}{subsection.2.5.1}{}}
\newlabel{sec:setup@cref}{{[subsection][1][5]5.1}{[1][7][]7}}
\citation{Saletore2018}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Class distribution for the 1684 training images used in our experiment.\relax }}{8}{figure.caption.5}}
\newlabel{fig:classdistribution}{{4}{8}{Class distribution for the 1684 training images used in our experiment.\relax }{figure.caption.5}{}}
\newlabel{fig:classdistribution@cref}{{[figure][4][]4}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Scaling up TTT in One Node with Dataset A}{8}{subsection.2.5.2}}
\newlabel{sec:scaleup}{{5.2}{8}{Scaling up TTT in One Node with Dataset A}{subsection.2.5.2}{}}
\newlabel{sec:scaleup@cref}{{[subsection][2][5]5.2}{[1][8][]8}}
\newlabel{fig:singlenodethroughput1w}{{5(a)}{9}{Throughput (in images/sec) -- 1 worker\relax }{figure.caption.6}{}}
\newlabel{fig:singlenodethroughput1w@cref}{{[subfigure][1][5]5(a)}{[1][8][]9}}
\newlabel{sub@fig:singlenodethroughput1w}{{(a)}{9}{Throughput (in images/sec) -- 1 worker\relax }{figure.caption.6}{}}
\newlabel{sub@fig:singlenodethroughput1w@cref}{{[subfigure][1][5]5(a)}{[1][8][]9}}
\newlabel{fig:singlenodethroughput4w}{{5(b)}{9}{Throughput (in images/sec) -- 4 workers\relax }{figure.caption.6}{}}
\newlabel{fig:singlenodethroughput4w@cref}{{[subfigure][2][5]5(b)}{[1][8][]9}}
\newlabel{sub@fig:singlenodethroughput4w}{{(b)}{9}{Throughput (in images/sec) -- 4 workers\relax }{figure.caption.6}{}}
\newlabel{sub@fig:singlenodethroughput4w@cref}{{[subfigure][2][5]5(b)}{[1][8][]9}}
\newlabel{fig:singlenodememutil1w}{{5(c)}{9}{Memory (in GB) -- 1 worker\relax }{figure.caption.6}{}}
\newlabel{fig:singlenodememutil1w@cref}{{[subfigure][3][5]5(c)}{[1][8][]9}}
\newlabel{sub@fig:singlenodememutil1w}{{(c)}{9}{Memory (in GB) -- 1 worker\relax }{figure.caption.6}{}}
\newlabel{sub@fig:singlenodememutil1w@cref}{{[subfigure][3][5]5(c)}{[1][8][]9}}
\newlabel{fig:singlenodememutil4w}{{5(d)}{9}{Memory (in GB) -- 4 workers\relax }{figure.caption.6}{}}
\newlabel{fig:singlenodememutil4w@cref}{{[subfigure][4][5]5(d)}{[1][8][]9}}
\newlabel{sub@fig:singlenodememutil4w}{{(d)}{9}{Memory (in GB) -- 4 workers\relax }{figure.caption.6}{}}
\newlabel{sub@fig:singlenodememutil4w@cref}{{[subfigure][4][5]5(d)}{[1][8][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  \textsf  {Throughput (in images/second) and memory utilized (in GB) with batch sizes 4 to 64 for 1 and 4 training workers respectively (a and b) on a single 2S Intel\textregistered {} Xeon\textregistered {} Gold 6148 processor with Dataset A.}\relax }}{9}{figure.caption.6}}
\newlabel{fig:multiworker}{{5}{9}{\textsf {Throughput (in images/second) and memory utilized (in GB) with batch sizes 4 to 64 for 1 and 4 training workers respectively (a and b) on a single 2S Intel\textregistered {} Xeon\textregistered {} Gold 6148 processor with Dataset A.}\relax }{figure.caption.6}{}}
\newlabel{fig:multiworker@cref}{{[figure][5][]5}{[1][8][]9}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textsf  {Two socket Intel\textregistered {} Xeon\textregistered {} Gold 6148 processor NUMA configuration}\relax }}{9}{figure.caption.7}}
\newlabel{fig:multisocket}{{6}{9}{\textsf {Two socket Intel\textregistered {} Xeon\textregistered {} Gold 6148 processor NUMA configuration}\relax }{figure.caption.7}{}}
\newlabel{fig:multisocket@cref}{{[figure][6][]6}{[1][8][]9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Scaling out TTT on 8 Servers with Dataset A}{10}{subsection.2.5.3}}
\newlabel{sec:scaleout_dataset1}{{5.3}{10}{Scaling out TTT on 8 Servers with Dataset A}{subsection.2.5.3}{}}
\newlabel{sec:scaleout_dataset1@cref}{{[subsection][3][5]5.3}{[1][10][]10}}
\newlabel{fig:loss8node}{{7(a)}{10}{Training loss\relax }{figure.caption.8}{}}
\newlabel{fig:loss8node@cref}{{[subfigure][1][7]7(a)}{[1][10][]10}}
\newlabel{sub@fig:loss8node}{{(a)}{10}{Training loss\relax }{figure.caption.8}{}}
\newlabel{sub@fig:loss8node@cref}{{[subfigure][1][7]7(a)}{[1][10][]10}}
\newlabel{fig:accuracy8node}{{7(b)}{10}{Validation accuracy\relax }{figure.caption.8}{}}
\newlabel{fig:accuracy8node@cref}{{[subfigure][2][7]7(b)}{[1][10][]10}}
\newlabel{sub@fig:accuracy8node}{{(b)}{10}{Validation accuracy\relax }{figure.caption.8}{}}
\newlabel{sub@fig:accuracy8node@cref}{{[subfigure][2][7]7(b)}{[1][10][]10}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textsf  {Training loss, top-1 and top-5 accuracy of M-CNN model with Dataset A in 30 epochs on 8x 2S Intel\textregistered {} Xeon\textregistered {} Gold 6148 processors connected with Intel\textregistered {} OP Fabric}\relax }}{10}{figure.caption.8}}
\newlabel{fig:8node}{{7}{10}{\textsf {Training loss, top-1 and top-5 accuracy of M-CNN model with Dataset A in 30 epochs on 8x 2S Intel\textregistered {} Xeon\textregistered {} Gold 6148 processors connected with Intel\textregistered {} OP Fabric}\relax }{figure.caption.8}{}}
\newlabel{fig:8node@cref}{{[figure][7][]7}{[1][10][]10}}
\expandafter \gdef \csname pgfplots@labelstyle@plotOne\endcsname {black={\pgfkeysnovalue },very thick={\pgfkeysnovalue }}
\expandafter \gdef \csname pgfplots@show@ref@plotOne\endcsname {\begingroup \def \pgfplots@draw@image {\def \tikz@plot@handler {\pgfkeys {/pgf/plots/@handler options/.cd, start=\relax , end macro=\relax , point macro=\pgfutil@gobble , jump macro=\relax , special macro=\pgfutil@gobble , point macro=\pgf@plot@line@handler , jump=\global \let \pgf@plotstreampoint \pgf@plot@line@handler@move }\begingroup \escapechar =-1 \edef \pgfplotsplothandlername {\string \pgfplothandlerlineto }\pgfmath@smuggleone \pgfplotsplothandlername \endgroup \def \pgfplotsplothandlerLUAfactory {function(axis, pointmetainputhandler) return pgfplots.GenericPlothandler.new("\pgfplotsplothandlername ", axis,pointmetainputhandler) end}\def \pgfplotsplothandlerLUAvisualizerfactory {pgfplots.defaultPlotVisualizerFactory}}\pgfkeysdef {/pgfplots/legend image code}{\draw [/pgfplots/mesh=false,bar width=3pt,bar shift=0pt,mark repeat=2,mark phase=2,] plot coordinates { (0cm,0cm) (0.3cm,0cm) (0.6cm,0cm)};}\pgfkeysdef {/pgfplots/every legend image post}{}\pgfkeyssetvalue {/pgfplots/mark list fill}{.!80!black}\pgfkeysdef {/pgfplots/every crossref picture}{\pgfkeysalso {baseline,yshift=0.3em}}\pgfplots@show@small@legendplots {black={\pgfkeysnovalue },very thick={\pgfkeysnovalue }}{}}\ifpgfpicture \scope [/pgfplots/every crossref picture]\pgfplots@draw@image \endscope \else \expandafter \ifx \csname tikzappendtofigurename\endcsname \relax \else \begingroup \tikzappendtofigurename {_crossref}\fi \tikz [/pgfplots/every crossref picture]{\pgfplots@draw@image }\expandafter \ifx \csname tikzappendtofigurename\endcsname \relax \else \endgroup \fi \fi \endgroup }
\expandafter \gdef \csname pgfplots@labelstyle@plotTwo\endcsname {orange={\pgfkeysnovalue },very thick={\pgfkeysnovalue }}
\expandafter \gdef \csname pgfplots@show@ref@plotTwo\endcsname {\begingroup \def \pgfplots@draw@image {\def \tikz@plot@handler {\pgfkeys {/pgf/plots/@handler options/.cd, start=\relax , end macro=\relax , point macro=\pgfutil@gobble , jump macro=\relax , special macro=\pgfutil@gobble , point macro=\pgf@plot@line@handler , jump=\global \let \pgf@plotstreampoint \pgf@plot@line@handler@move }\begingroup \escapechar =-1 \edef \pgfplotsplothandlername {\string \pgfplothandlerlineto }\pgfmath@smuggleone \pgfplotsplothandlername \endgroup \def \pgfplotsplothandlerLUAfactory {function(axis, pointmetainputhandler) return pgfplots.GenericPlothandler.new("\pgfplotsplothandlername ", axis,pointmetainputhandler) end}\def \pgfplotsplothandlerLUAvisualizerfactory {pgfplots.defaultPlotVisualizerFactory}}\pgfkeysdef {/pgfplots/legend image code}{\draw [/pgfplots/mesh=false,bar width=3pt,bar shift=0pt,mark repeat=2,mark phase=2,] plot coordinates { (0cm,0cm) (0.3cm,0cm) (0.6cm,0cm)};}\pgfkeysdef {/pgfplots/every legend image post}{}\pgfkeyssetvalue {/pgfplots/mark list fill}{.!80!black}\pgfkeysdef {/pgfplots/every crossref picture}{\pgfkeysalso {baseline,yshift=0.3em}}\pgfplots@show@small@legendplots {orange={\pgfkeysnovalue },very thick={\pgfkeysnovalue }}{}}\ifpgfpicture \scope [/pgfplots/every crossref picture]\pgfplots@draw@image \endscope \else \expandafter \ifx \csname tikzappendtofigurename\endcsname \relax \else \begingroup \tikzappendtofigurename {_crossref}\fi \tikz [/pgfplots/every crossref picture]{\pgfplots@draw@image }\expandafter \ifx \csname tikzappendtofigurename\endcsname \relax \else \endgroup \fi \fi \endgroup }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textsf  {Scaling M-CNN training with Dataset A from 1X to 8X 2S Intel\textregistered {} Xeon\textregistered {} Gold 6148 processors connected with 100Gbps Intel\textregistered {} OP Fabric}\relax }}{11}{figure.caption.9}}
\newlabel{fig:scaleout_dataset1}{{8}{11}{\textsf {Scaling M-CNN training with Dataset A from 1X to 8X 2S Intel\textregistered {} Xeon\textregistered {} Gold 6148 processors connected with 100Gbps Intel\textregistered {} OP Fabric}\relax }{figure.caption.9}{}}
\newlabel{fig:scaleout_dataset1@cref}{{[figure][8][]8}{[1][10][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Scaling out TTT on 128 Servers with Dataset B}{11}{subsection.2.5.4}}
\newlabel{sec:scaleout_dataset2}{{5.4}{11}{Scaling out TTT on 128 Servers with Dataset B}{subsection.2.5.4}{}}
\newlabel{sec:scaleout_dataset2@cref}{{[subsection][4][5]5.4}{[1][10][]11}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces M-CNN Training Performance on 128 2S Intel\textregistered {} Xeon\textregistered {} Gold processors with Dataset B\relax }}{11}{table.caption.10}}
\newlabel{table:mcnn128nodes}{{1}{11}{M-CNN Training Performance on 128 2S Intel\textregistered {} Xeon\textregistered {} Gold processors with Dataset B\relax }{table.caption.10}{}}
\newlabel{table:mcnn128nodes@cref}{{[table][1][]1}{[1][10][]11}}
\bibstyle{IEEEtran}
\bibdata{bibliography}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{12}{section.2.6}}
\newlabel{sec:discussion}{{6}{12}{Discussion}{section.2.6}{}}
\newlabel{sec:discussion@cref}{{[section][6][]6}{[1][12][]12}}
\bibcite{Arbabshirani2018}{1}
\bibcite{Akkus2017}{2}
\bibcite{ciresan2013}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Top-1 Accuracy achieved in 20 epochs of M-CNN training and Learning Rate used on 1--64 2S Intel\textregistered {} Xeon\textregistered {} Gold processors. Dataset B is used for these experiments. Global minibatch size is capped at 2K from 16 to 64 nodes. The learning rate as shown in (f) -- (h) is also scaled only to 0.032 to achieve convergence\relax }}{13}{figure.caption.11}}
\newlabel{fig:top1acc_dataset2}{{9}{13}{Top-1 Accuracy achieved in 20 epochs of M-CNN training and Learning Rate used on 1--64 2S Intel\textregistered {} Xeon\textregistered {} Gold processors. Dataset B is used for these experiments. Global minibatch size is capped at 2K from 16 to 64 nodes. The learning rate as shown in (f) -- (h) is also scaled only to 0.032 to achieve convergence\relax }{figure.caption.11}{}}
\newlabel{fig:top1acc_dataset2@cref}{{[figure][9][]9}{[1][11][]13}}
\bibcite{Litjens2016}{4}
\bibcite{Janowczyk2016}{5}
\bibcite{Kraus2017}{6}
\bibcite{Sommer2017}{7}
\bibcite{Ciresan2012}{8}
\bibcite{Litjens2017}{9}
\bibcite{Mattiazzi2016}{10}
\newlabel{plotOne}{{\expandafter\protect\csname pgfplots@show@ref@plotOne\endcsname}{14}{Scaling out TTT on 128 Servers with Dataset B}{pgfplotslink.2.5.0}{}}
\newlabel{plotOne@cref}{{[subsection][4][5]5.4}{[1][11][]14}}
\newlabel{plotTwo}{{\expandafter\protect\csname pgfplots@show@ref@plotTwo\endcsname}{14}{Scaling out TTT on 128 Servers with Dataset B}{pgfplotslink.2.5.1}{}}
\newlabel{plotTwo@cref}{{[subsection][4][5]5.4}{[1][11][]14}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Scalability of M-CNN training performance for 20 epochs on 64 2S Intel\textregistered {} Xeon\textregistered {} Gold 6148 processors. Note that global batch size is capped at 2K from 16 -- 64 nodes. Intel\textregistered {} OP Fabric, TensorFlow-1.9.0+Horovod, OpenMPI v3.0.0, 8 workers/node\relax }}{14}{figure.caption.12}}
\newlabel{fig:20epoch_dataset2}{{10}{14}{Scalability of M-CNN training performance for 20 epochs on 64 2S Intel\textregistered {} Xeon\textregistered {} Gold 6148 processors. Note that global batch size is capped at 2K from 16 -- 64 nodes. Intel\textregistered {} OP Fabric, TensorFlow-1.9.0+Horovod, OpenMPI v3.0.0, 8 workers/node\relax }{figure.caption.12}{}}
\newlabel{fig:20epoch_dataset2@cref}{{[figure][10][]10}{[1][11][]14}}
\bibcite{Boutros2015}{11}
\bibcite{Singh2014}{12}
\bibcite{Scheeder2018}{13}
\bibcite{zock2009}{14}
\bibcite{assay2014}{15}
\bibcite{Godinez2017}{16}
\bibcite{Godinez2018}{17}
\bibcite{Ando2017}{18}
\bibcite{Buyssens2012}{19}
\bibcite{jouppi2017}{20}
\bibcite{Robbins1951}{21}
\bibcite{You2017}{22}
\bibcite{bbbc021}{23}
\bibcite{Caie1913}{24}
\bibcite{GoogleTPU}{25}
\bibcite{Horovod}{26}
\bibcite{Saletore2018}{27}
